{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California housing dataset regression with MLPs\n",
    "\n",
    "In this notebook, we'll train a multi-layer perceptron model to to estimate median house values on Californian housing districts.\n",
    "\n",
    "First, the needed imports. Keras tells us which backend (Theano, Tensorflow, CNTK) it will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "from keras import __version__\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "assert(LV(__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Then we load the California housing data. First time we need to download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd = datasets.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of 20640 housing districts, each characterized with 8 attributes: *MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude*. There is also a target value (median house value) for each housing district.\n",
    " \n",
    "Let's plot all attributes against the target value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i in range(8):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    plt.scatter(chd.data[:,i], chd.target, s=2, label=chd.feature_names[i])\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now split the data into a training and a test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 5000\n",
    "\n",
    "X_train_all, X_test_all, y_train, y_test = train_test_split(\n",
    "    chd.data, chd.target, test_size=test_size, shuffle=True)\n",
    "\n",
    "X_train_single = X_train_all[:,0].reshape(-1, 1)\n",
    "X_test_single = X_test_all[:,0].reshape(-1, 1)\n",
    "     \n",
    "print()\n",
    "print('California housing data: train:',len(X_train_all),'test:',len(X_test_all))\n",
    "print()\n",
    "print('X_train_all:', X_train_all.shape)\n",
    "print('X_train_single:', X_train_single.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print()\n",
    "print('X_test_all', X_test_all.shape)\n",
    "print('X_test_single', X_test_single.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data matrix `X_train_all` is a matrix of size (`n_train`, 8), and `X_train_single` contains only the first attribute *(MedInc)*. `y_train` is a vector containing the target value (median house value) for each housing district in the training set.\n",
    "\n",
    "Let's start our analysis with a single attribute *(MedInc)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_single\n",
    "X_test = X_test_single\n",
    "\n",
    "#X_train = X_train_all\n",
    "#X_test = X_test_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final step, let's scale the input data to zero mean and unit variance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print('X_train: mean:', X_train.mean(axis=0), 'std:', X_train.std(axis=0))\n",
    "print('X_test: mean:', X_test.mean(axis=0), 'std:', X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hidden layer\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Let's begin with a simple model that has a single hidden layer.  We first initialize the model with `Sequential()`.  Then we add a `Dense` layer that has `X_train.shape[1]` inputs (one for each attribute in the training data) and 10 units. The `Dense` layer connects each input to each output with some weight parameter. \n",
    "Then we have an output layer that has only one unit with a linear activation function.\n",
    "\n",
    "Finally, we select *mean squared error* as the loss function, select [*stochastic gradient descent*](https://keras.io/optimizers/#sgd) as the optimizer, and `compile()` the model. Note there are [several different options](https://keras.io/optimizers/) for the optimizer in Keras that we could use instead of *sgd*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel = Sequential()\n",
    "linmodel.add(Dense(units=10, input_dim=X_train.shape[1], activation='relu'))\n",
    "linmodel.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "linmodel.compile(loss='mean_squared_error', \n",
    "                 optimizer='sgd')\n",
    "print(linmodel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw a fancier graph of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(linmodel, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Now we are ready to train our first model.  An *epoch* means one pass through the whole training data. \n",
    "\n",
    "You can run code below multiple times and it will continue the training process from where it left off.  If you want to start from scratch, re-initialize the model using the code a few cells ago. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10 \n",
    "\n",
    "linhistory = linmodel.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=epochs, \n",
    "                          batch_size=32,\n",
    "                          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how the training progressed. *Loss* is a function of the difference of the network output and the target values.  We are minimizing the loss function during training so it should decrease over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(linhistory.epoch,linhistory.history['loss'])\n",
    "plt.title('loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train.shape[1] == 1:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(X_train, y_train, s=5)\n",
    "    reg_x = np.arange(np.min(X_train), np.max(X_train), 0.01).reshape(-1, 1)\n",
    "    plt.scatter(reg_x, linmodel.predict(reg_x), s=8, label='one hidden layer')\n",
    "    plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the model accuracy for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = linmodel.predict(X_test)\n",
    "print(\"Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Let's now create a more complex MLP model that has multiple dense layers and dropout layers.  `Dropout()` randomly sets a fraction of inputs to zero during training, which is one approach to regularization and can sometimes help to prevent overfitting.\n",
    "\n",
    "The last layer needs to have a single unit with linear activation to match the groundtruth (`Y_train`). \n",
    "\n",
    "Finally, we again `compile()` the model, this time using [*Adam*](https://keras.io/optimizers/#adam) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel = Sequential()\n",
    "\n",
    "mlmodel.add(Dense(units=20, input_dim=X_train.shape[1], activation='relu'))\n",
    "mlmodel.add(Dense(units=20, activation='relu'))\n",
    "mlmodel.add(Dropout(0.5))\n",
    "\n",
    "mlmodel.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "mlmodel.compile(loss='mean_squared_error', \n",
    "                optimizer='adam')\n",
    "print(mlmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(mlmodel, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10 \n",
    "\n",
    "history = mlmodel.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=32,\n",
    "                      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train.shape[1] == 1:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(X_train, y_train, s=5)\n",
    "    reg_x = np.arange(np.min(X_train), np.max(X_train), 0.01).reshape(-1, 1)\n",
    "    plt.scatter(reg_x, linmodel.predict(reg_x), s=8, label='one hidden layer')\n",
    "    plt.scatter(reg_x, mlmodel.predict(reg_x), s=8, label='multiple hidden layers')\n",
    "    plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = mlmodel.predict(X_test)\n",
    "print(\"Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "Try to reduce the mean squared error of the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
